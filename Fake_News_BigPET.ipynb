{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hq6euCBejaSQ",
        "outputId": "a5c3907e-664f-4061-a437-10b98919f3d1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "import string\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer \n",
        "from wordcloud import WordCloud, STOPWORDS\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier,GradientBoostingClassifier\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ciDIsDj2R52c",
        "outputId": "19174a2a-0629-4bbd-b3b7-81c34b78589d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Importação completa!\n"
          ]
        }
      ],
      "source": [
        "#Criando conjunto de \"Dados Estruturados\"\n",
        "\n",
        "# Importando Dados para o COLAB para fazer a montagem do drive no notebook\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "print('Importação completa!')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4jUdiETjSDBU"
      },
      "outputs": [],
      "source": [
        "#Notícias Verdadeiras Drive PET\n",
        "X = []\n",
        "y = []\n",
        "\n",
        "N = 3600 # Número de notícias coletadas do banco de \"notícias genuínas\"\n",
        "\n",
        "i = 0\n",
        "while i < N:\n",
        "    try:\n",
        "        file = open(\"/content/drive/MyDrive/Projetos/Big PET - Data Science/Detector de FakeNews/dataset/true/\"+str(i+1)+\".txt\")\n",
        "        # Os arquivos contidos na pasta apresentam como identidade um número mais a extensão .txt\n",
        "        X.append(file.readlines())\n",
        "        y.append(True)\n",
        "        file.close()\n",
        "    except:\n",
        "        print('Erro na leitura ou ausência do arquivo: ', i)\n",
        "        # Sequencialmente, cada arquivos será checado, um a um\n",
        "        X.append([None])\n",
        "        y.append([None])\n",
        "    i += 1\n",
        "\n",
        "news = pd.DataFrame(data = list(zip(X, y)), columns = ['Conteúdo', 'Veredicto'])\n",
        "news.index = np.arange(len(news))     \n",
        "print('Validação completa')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "BrJAVKWWF7zW",
        "outputId": "eb123d50-2be1-4b29-84df-40db6a774ca2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Erro na leitura ou ausência do arquivo:  585\n",
            "Erro na leitura ou ausência do arquivo:  1606\n",
            "Validação completa\n"
          ]
        }
      ],
      "source": [
        "#Notícias Verdadeiras Drive Leonardo\n",
        "X = []\n",
        "y = []\n",
        "\n",
        "N = 3600 # Número de notícias coletadas do banco de \"notícias genuínas\"\n",
        "\n",
        "i = 0\n",
        "while i < N:\n",
        "    try:\n",
        "        file = open(\"/content/drive/MyDrive/PET/Detector de FakeNews/dataset/true/\"+str(i+1)+\".txt\")\n",
        "        # Os arquivos contidos na pasta apresentam como identidade um número mais a extensão .txt\n",
        "        X.append(file.readlines())\n",
        "        y.append(True)\n",
        "        file.close()\n",
        "    except:\n",
        "        print('Erro na leitura ou ausência do arquivo: ', i)\n",
        "        # Sequencialmente, cada arquivos será checado, um a um\n",
        "        X.append([None])\n",
        "        y.append([None])\n",
        "    i += 1\n",
        "\n",
        "news = pd.DataFrame(data = list(zip(X, y)), columns = ['Conteúdo', 'Veredicto'])\n",
        "news.index = np.arange(len(news))     \n",
        "print('Validação completa')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MeIbPgNabf0u"
      },
      "outputs": [],
      "source": [
        "#Tratando os Dados\n",
        "#Subsituição de caracteres invalidos das noticias verdadeiras \n",
        "news_np = pd.DataFrame.to_numpy(news['Conteúdo'])\n",
        "lista = []\n",
        "for i in range(len(news_np)):\n",
        "  lista.append(news_np[i])\n",
        "  if lista[i][0] != None:\n",
        "    lista[i][0] = lista[i][0].replace('\\n', '') #Removendo/alterando caracteres\n",
        "    lista[i][0] = lista[i][0].replace('é', 'e')\n",
        "    lista[i][0] = lista[i][0].replace('ê', 'e')\n",
        "    lista[i][0] = lista[i][0].replace('á', 'a')\n",
        "    lista[i][0] = lista[i][0].replace('â', 'a')\n",
        "    lista[i][0] = lista[i][0].replace('à', 'a')\n",
        "    lista[i][0] = lista[i][0].replace('ã', 'a')\n",
        "    lista[i][0] = lista[i][0].replace('õ', 'o')\n",
        "    lista[i][0] = lista[i][0].replace('ó', 'o')\n",
        "    lista[i][0] = lista[i][0].replace('ô', 'o')\n",
        "    lista[i][0] = lista[i][0].replace('ú', 'u')\n",
        "    lista[i][0] = lista[i][0].replace('í', 'i')\n",
        "    lista[i][0] = lista[i][0].replace('ç', 'c') \n",
        "  lista[i] = lista[i][0]\n",
        "news['Conteúdo'] = lista\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359
        },
        "id": "igJ6uZqyVLvF",
        "outputId": "a85a191a-fa20-4841-a585-e7b756e7f1b6"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Conteúdo</th>\n",
              "      <th>Veredicto</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>O Podemos decidiu  expulsar o deputado federal...</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Em evento realizado nesta terca-feira para div...</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>﻿Apos o prefeito de Manaus Arthur Virgilio (PS...</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>﻿Doria vai receber Ze Celso apos reuniao com r...</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Gustavo Pedreira Ferraz, que admitiu buscar ma...</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Muito antes da Operacao Lava Jato existir, em ...</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>Ministerio Publico do Maranhao (MP-MA) ingress...</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>Cuba comemorara o primeiro aniversario da mort...</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>s doleiros acusados de lavar dinheiro roubado ...</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>Bolsonaro e um liberal completo, diz president...</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                            Conteúdo Veredicto\n",
              "0  O Podemos decidiu  expulsar o deputado federal...      True\n",
              "1  Em evento realizado nesta terca-feira para div...      True\n",
              "2  ﻿Apos o prefeito de Manaus Arthur Virgilio (PS...      True\n",
              "3  ﻿Doria vai receber Ze Celso apos reuniao com r...      True\n",
              "4  Gustavo Pedreira Ferraz, que admitiu buscar ma...      True\n",
              "5  Muito antes da Operacao Lava Jato existir, em ...      True\n",
              "6  Ministerio Publico do Maranhao (MP-MA) ingress...      True\n",
              "7  Cuba comemorara o primeiro aniversario da mort...      True\n",
              "8  s doleiros acusados de lavar dinheiro roubado ...      True\n",
              "9  Bolsonaro e um liberal completo, diz president...      True"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Dataset de notícias verdadeiras\n",
        "\n",
        "news.head(10)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dySeo2h4P3lR",
        "outputId": "a9c988b6-5ade-437f-a9d8-625e48e92865"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Erro na leitura ou ausência do arquivo:  585\n",
            "Erro na leitura ou ausência do arquivo:  1606\n"
          ]
        }
      ],
      "source": [
        "#Notícias Falsas\n",
        "X = []\n",
        "y = []\n",
        "\n",
        "N = 3600  # Número de notícias coletadas do banco de \"notícias falsas\"\n",
        "i = 0\n",
        "while i < N:\n",
        "    try:\n",
        "        file = open(\"/content/drive/MyDrive/PET/Detector de FakeNews/dataset/fake/\"+str(i+1)+\".txt\")\n",
        "        # Os arquivos contidos na pasta apresentam como identidade um número mais a extensão .txt\n",
        "        X.append(file.readlines())\n",
        "        y.append(False)\n",
        "        file.close()\n",
        "    except:\n",
        "        print('Erro na leitura ou ausência do arquivo: ', i)\n",
        "        # Sequencialmente, cada arquivos será checado, um a um\n",
        "        X.append([None])\n",
        "        y.append([None])\n",
        "    i += 1\n",
        "fakenews = pd.DataFrame(data = list(zip(X, y)), columns = ['Conteúdo', 'Veredicto'])\n",
        "fakenews.index = np.arange(len(fakenews)) + len(news)\n",
        "print('Validação completa')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s-PnWr_oaLeT"
      },
      "outputs": [],
      "source": [
        "#Tratando os Dados\n",
        "#Subsituição de caracteres invalidos das noticias falsas\n",
        "news_np = pd.DataFrame.to_numpy(fakenews['Conteúdo'])\n",
        "lista = []\n",
        "for i in range(len(news_np)):\n",
        "  lista.append(news_np[i])\n",
        "  if lista[i][0] != None:\n",
        "    lista[i][0] = lista[i][0].replace('\\n', '') #Removendo/alterando caracteres\n",
        "    lista[i][0] = lista[i][0].replace('é', 'e')\n",
        "    lista[i][0] = lista[i][0].replace('ê', 'e')\n",
        "    lista[i][0] = lista[i][0].replace('á', 'a')\n",
        "    lista[i][0] = lista[i][0].replace('â', 'a')\n",
        "    lista[i][0] = lista[i][0].replace('à', 'a')\n",
        "    lista[i][0] = lista[i][0].replace('ã', 'a')\n",
        "    lista[i][0] = lista[i][0].replace('õ', 'o')\n",
        "    lista[i][0] = lista[i][0].replace('ó', 'o')\n",
        "    lista[i][0] = lista[i][0].replace('ô', 'o')\n",
        "    lista[i][0] = lista[i][0].replace('ú', 'u')\n",
        "    lista[i][0] = lista[i][0].replace('í', 'i')\n",
        "    lista[i][0] = lista[i][0].replace('ç', 'c') \n",
        "  lista[i] = lista[i][0]\n",
        "fakenews['Conteúdo'] = lista\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0fNYzoLYv9zz"
      },
      "outputs": [],
      "source": [
        "# Dataset de notícias falsas\n",
        "\n",
        "fakenews.head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BAfQsbSmcMSL"
      },
      "outputs": [],
      "source": [
        "# Removendo os poucos valores inválidos\n",
        "news = news.dropna() \n",
        "fakenews = fakenews.dropna()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GwwQ7DzRVOWa"
      },
      "outputs": [],
      "source": [
        "news.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kA3byPj9cIOE"
      },
      "outputs": [],
      "source": [
        "news.count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4hRyAS-aTOU9"
      },
      "outputs": [],
      "source": [
        "fakenews.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qZY0OcI0TQ-W"
      },
      "outputs": [],
      "source": [
        "fakenews.count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fca3Gx-chkZj"
      },
      "outputs": [],
      "source": [
        "# Unindo conjunto fakenews e news\n",
        "df = pd.concat([news, fakenews], ignore_index = True) \n",
        "df.index = np.arange(len(df))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UA7XL1IgiXgU"
      },
      "outputs": [],
      "source": [
        "# Importando palavras mais comuns do Português\n",
        "\n",
        "file = open('/content/drive/MyDrive/Detector de FakeNews/stopwords.txt')\n",
        "stopwords = file.readlines()\n",
        "for i in range(len(stopwords)):\n",
        "  stopwords[i] = stopwords[i].replace(' ', '')\n",
        "  stopwords[i] = stopwords[i].replace('\\n', '')\n",
        "  stopwords[i] = stopwords[i].replace('é', 'e')\n",
        "  stopwords[i] = stopwords[i].replace('ê', 'e')\n",
        "  stopwords[i] = stopwords[i].replace('á', 'a')\n",
        "  stopwords[i] = stopwords[i].replace('â', 'a')\n",
        "  stopwords[i] = stopwords[i].replace('à', 'a')\n",
        "  stopwords[i] = stopwords[i].replace('ã', 'a')\n",
        "  stopwords[i] = stopwords[i].replace('õ', 'o')\n",
        "  stopwords[i] = stopwords[i].replace('ô', 'o')\n",
        "  stopwords[i] = stopwords[i].replace('ó', 'o')\n",
        "  stopwords[i] = stopwords[i].replace('ú', 'u')\n",
        "  stopwords[i] = stopwords[i].replace('í', 'i')\n",
        "  stopwords[i] = stopwords[i].replace('ç', 'c') \n",
        "file.close()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fn7RI8m4jW04"
      },
      "outputs": [],
      "source": [
        "stopwords = stopwords[:50] "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LmliVsvHcmLq"
      },
      "outputs": [],
      "source": [
        "#definindo função que remove as stopwords do texto\n",
        "stop = stopwords\n",
        "def remove_stopwords(text):\n",
        "    final_text = []\n",
        "    text = text.lower()\n",
        "    for i in text.split():\n",
        "        if i.strip() not in stop:\n",
        "            final_text.append(i.strip())\n",
        "    return \" \".join(final_text)\n",
        "\n",
        "#aplicaando a função remove_stopwords\n",
        "df['Conteúdo']=df['Conteúdo'].apply(remove_stopwords)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M1hWE5Icge9H"
      },
      "outputs": [],
      "source": [
        "#lemmatization\n",
        "# Init the Wordnet Lemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "#A function which takes a sentence/corpus and gets its lemmatized version.\n",
        "def lemmatize_text(text):\n",
        "    token_words=word_tokenize(text) \n",
        "#we need to tokenize the sentence or else lemmatizing will return the entire sentence as is.\n",
        "    lemma_sentence=[]\n",
        "    for word in token_words:\n",
        "        lemma_sentence.append(lemmatizer.lemmatize(word))\n",
        "        lemma_sentence.append(\" \")\n",
        "    return \"\".join(lemma_sentence)\n",
        "\n",
        "#Apply function on text column\n",
        "df['Conteúdo']=df['Conteúdo'].apply(lemmatize_text)\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lyMgizGYiK6x"
      },
      "outputs": [],
      "source": [
        "#word cloud para as fake news\n",
        "cloud = WordCloud(max_words = 500, stopwords = STOPWORDS, background_color = \"white\").generate(\" \".join(df[df.Veredicto == True].Conteúdo))\n",
        "plt.figure(figsize=(10, 10))\n",
        "plt.imshow(cloud, interpolation=\"bilinear\")\n",
        "plt.axis(\"off\")\n",
        "plt.tight_layout(pad=0)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7DCmQgGCkbAI"
      },
      "outputs": [],
      "source": [
        "cloud = WordCloud(max_words = 500, stopwords = STOPWORDS, background_color = \"white\").generate(\" \".join(df[df.Veredicto == False].Conteúdo))\n",
        "plt.figure(figsize=(10, 10))\n",
        "plt.imshow(cloud, interpolation=\"bilinear\")\n",
        "plt.axis(\"off\")\n",
        "plt.tight_layout(pad=0)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qq_z19tpjKqT"
      },
      "outputs": [],
      "source": [
        "#finding n-grams\n",
        "texts = ''.join(str(df['Conteúdo'].tolist()))\n",
        "\n",
        "# first get individual words\n",
        "tokenized = texts.split()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g_TvPix_jT3d"
      },
      "outputs": [],
      "source": [
        "#unigram\n",
        "unigram = (pd.Series(nltk.ngrams(tokenized, 1)).value_counts())[:20]\n",
        "unigram.sort_values().plot.barh(width=.9, figsize=(12, 8))\n",
        "plt.title('20 Most Frequently Occuring Unigrams')\n",
        "plt.ylabel('Unigram')\n",
        "plt.xlabel('# of Occurances')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OVk_2jwcjude"
      },
      "outputs": [],
      "source": [
        "#bigrams\n",
        "bigram = (pd.Series(nltk.ngrams(tokenized, 2)).value_counts())[:20]\n",
        "bigram.sort_values().plot.barh(width=.9, figsize=(12, 8))\n",
        "plt.title('20 Most Frequently Occuring Bigrams')\n",
        "plt.ylabel('Bigram')\n",
        "plt.xlabel('# of Occurances')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HkErdfgQmjp3"
      },
      "outputs": [],
      "source": [
        "#modeling\n",
        "def get_prediction(vectorizer, classifier, X_train, X_test, y_train, y_test):\n",
        "    pipe = Pipeline([('vector', vectorizer),\n",
        "                    ('model', classifier)])\n",
        "    model = pipe.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "    print(\"Accuarcy: {}\".format(round(accuracy_score(y_test, y_pred)*100,2)))\n",
        "    cm = confusion_matrix(y_test, y_pred)\n",
        "    print(\"Confusion Matrix: \\n\", cm)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3PvGYFp0mlB3"
      },
      "outputs": [],
      "source": [
        "#pipeline implementation\n",
        "Conteudo = df['Conteúdo']\n",
        "Conteudo = Conteudo.astype(str)\n",
        "\n",
        "Veredicto = df['Veredicto']\n",
        "Veredicto = Veredicto.astype(bool)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(Conteudo, Veredicto, test_size = 0.3, random_state= 0)\n",
        "classifiers = [LogisticRegression(),KNeighborsClassifier(n_neighbors=5), DecisionTreeClassifier(), GradientBoostingClassifier(),RandomForestClassifier()]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CaB_rjiC7IfZ"
      },
      "outputs": [],
      "source": [
        "Conteudo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NOoeytZUn75a"
      },
      "outputs": [],
      "source": [
        "for classifier in classifiers:\n",
        "    get_prediction(TfidfVectorizer(), classifier, X_train, X_test, y_train, y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PH_3ZpWG8y-f"
      },
      "outputs": [],
      "source": [
        "dataBase = pd.DataFrame(data = list(zip(Conteudo, Veredicto)), columns = ['Conteúdo', 'Veredicto'])\n",
        "dataBase"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Fake News BigPET.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}